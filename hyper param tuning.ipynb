{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jjwjjwjjwjjwjjw/Home-Credit-Default-Risk/blob/main/hyper%20param%20tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEHCvFhIvR4F"
      },
      "source": [
        "### Bayesian Optimization을 이용하여 application과 previous로 만들어진 집합의 하이퍼 파라미터 튜닝"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eg1AO0nNvR4H"
      },
      "source": [
        "#### 라이브러리 및 데이터 세트 로딩. 이전 application 데이터의 FE 함수 복사"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "il0VyvcgvR4K"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gc\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "%matplotlib inline\n",
        "\n",
        "pd.set_option('display.max_rows', 100)\n",
        "pd.set_option('display.max_columns', 200)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIJ5-Lu-wbya"
      },
      "source": [
        "##### 코랩 버전은 Google Drive에서 데이터 세트를 로딩"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHiS9-DqwUk_",
        "outputId": "2de72119-7f17-4e1d-b7a0-db85a93e2208"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import os, sys\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vaO4thuSvR4X"
      },
      "outputs": [],
      "source": [
        "def get_dataset():\n",
        "    default_dir = \"/content/gdrive/My Drive\"\n",
        "    app_train = pd.read_csv(os.path.join(default_dir, 'application_train.csv'))\n",
        "    app_test = pd.read_csv(os.path.join(default_dir, 'application_test.csv'))\n",
        "    apps = pd.concat([app_train, app_test])\n",
        "    prev = pd.read_csv(os.path.join(default_dir, 'previous_application.csv'))\n",
        "\n",
        "    return apps, prev\n",
        "\n",
        "apps, prev = get_dataset()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmG4YOdpvR4e"
      },
      "source": [
        "#### 이전 application 데이터의 feature engineering 함수 복사"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2WIrIw4uvR4h"
      },
      "outputs": [],
      "source": [
        "def get_apps_processed(apps):\n",
        "\n",
        "    # EXT_SOURCE_X FEATURE 가공\n",
        "    apps['APPS_EXT_SOURCE_MEAN'] = apps[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].mean(axis=1)\n",
        "    apps['APPS_EXT_SOURCE_STD'] = apps[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].std(axis=1)\n",
        "    apps['APPS_EXT_SOURCE_STD'] = apps['APPS_EXT_SOURCE_STD'].fillna(apps['APPS_EXT_SOURCE_STD'].mean())\n",
        "\n",
        "    # AMT_CREDIT 비율로 Feature 가공\n",
        "    apps['APPS_ANNUITY_CREDIT_RATIO'] = apps['AMT_ANNUITY']/apps['AMT_CREDIT']\n",
        "    apps['APPS_GOODS_CREDIT_RATIO'] = apps['AMT_GOODS_PRICE']/apps['AMT_CREDIT']\n",
        "\n",
        "    # AMT_INCOME_TOTAL 비율로 Feature 가공\n",
        "    apps['APPS_ANNUITY_INCOME_RATIO'] = apps['AMT_ANNUITY']/apps['AMT_INCOME_TOTAL']\n",
        "    apps['APPS_CREDIT_INCOME_RATIO'] = apps['AMT_CREDIT']/apps['AMT_INCOME_TOTAL']\n",
        "    apps['APPS_GOODS_INCOME_RATIO'] = apps['AMT_GOODS_PRICE']/apps['AMT_INCOME_TOTAL']\n",
        "    apps['APPS_CNT_FAM_INCOME_RATIO'] = apps['AMT_INCOME_TOTAL']/apps['CNT_FAM_MEMBERS']\n",
        "\n",
        "    # DAYS_BIRTH, DAYS_EMPLOYED 비율로 Feature 가공\n",
        "    apps['APPS_EMPLOYED_BIRTH_RATIO'] = apps['DAYS_EMPLOYED']/apps['DAYS_BIRTH']\n",
        "    apps['APPS_INCOME_EMPLOYED_RATIO'] = apps['AMT_INCOME_TOTAL']/apps['DAYS_EMPLOYED']\n",
        "    apps['APPS_INCOME_BIRTH_RATIO'] = apps['AMT_INCOME_TOTAL']/apps['DAYS_BIRTH']\n",
        "    apps['APPS_CAR_BIRTH_RATIO'] = apps['OWN_CAR_AGE'] / apps['DAYS_BIRTH']\n",
        "    apps['APPS_CAR_EMPLOYED_RATIO'] = apps['OWN_CAR_AGE'] / apps['DAYS_EMPLOYED']\n",
        "\n",
        "    return apps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mU0PqE9vR4o"
      },
      "source": [
        "#### previous 데이터 가공후 인코딩 및 최종 데이터 집합 생성하는 함수 선언"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VOSuDuMvvR4p"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "def get_prev_processed(prev):\n",
        "    # 대출 신청 금액과 실제 대출액/대출 상품금액 차이 및 비율\n",
        "    prev['PREV_CREDIT_DIFF'] = prev['AMT_APPLICATION'] - prev['AMT_CREDIT']\n",
        "    prev['PREV_GOODS_DIFF'] = prev['AMT_APPLICATION'] - prev['AMT_GOODS_PRICE']\n",
        "    prev['PREV_CREDIT_APPL_RATIO'] = prev['AMT_CREDIT']/prev['AMT_APPLICATION']\n",
        "    # prev['PREV_ANNUITY_APPL_RATIO'] = prev['AMT_ANNUITY']/prev['AMT_APPLICATION']\n",
        "    prev['PREV_GOODS_APPL_RATIO'] = prev['AMT_GOODS_PRICE']/prev['AMT_APPLICATION']\n",
        "\n",
        "    prev['DAYS_FIRST_DRAWING'].replace(365243, np.nan, inplace= True)\n",
        "    prev['DAYS_FIRST_DUE'].replace(365243, np.nan, inplace= True)\n",
        "    prev['DAYS_LAST_DUE_1ST_VERSION'].replace(365243, np.nan, inplace= True)\n",
        "    prev['DAYS_LAST_DUE'].replace(365243, np.nan, inplace= True)\n",
        "    prev['DAYS_TERMINATION'].replace(365243, np.nan, inplace= True)\n",
        "    # 첫번째 만기일과 마지막 만기일까지의 기간\n",
        "    prev['PREV_DAYS_LAST_DUE_DIFF'] = prev['DAYS_LAST_DUE_1ST_VERSION'] - prev['DAYS_LAST_DUE']\n",
        "    # 매월 납부 금액과 납부 횟수 곱해서 전체 납부 금액 구함.\n",
        "    all_pay = prev['AMT_ANNUITY'] * prev['CNT_PAYMENT']\n",
        "    # 전체 납부 금액 대비 AMT_CREDIT 비율을 구하고 여기에 다시 납부횟수로 나누어서 이자율 계산.\n",
        "    prev['PREV_INTERESTS_RATE'] = (all_pay/prev['AMT_CREDIT'] - 1)/prev['CNT_PAYMENT']\n",
        "\n",
        "    return prev\n",
        "\n",
        "\n",
        "def get_prev_amt_agg(prev):\n",
        "    # 새롭게 생성된 대출 신청액 대비 다른 금액 차이 및 비율로 aggregation 수행.\n",
        "    agg_dict = {\n",
        "         # 기존 컬럼.\n",
        "        'SK_ID_CURR':['count'],\n",
        "        'AMT_CREDIT':['mean', 'max', 'sum'],\n",
        "        'AMT_ANNUITY':['mean', 'max', 'sum'],\n",
        "        'AMT_APPLICATION':['mean', 'max', 'sum'],\n",
        "        'AMT_DOWN_PAYMENT':['mean', 'max', 'sum'],\n",
        "        'AMT_GOODS_PRICE':['mean', 'max', 'sum'],\n",
        "        'RATE_DOWN_PAYMENT': ['min', 'max', 'mean'],\n",
        "        'DAYS_DECISION': ['min', 'max', 'mean'],\n",
        "        'CNT_PAYMENT': ['mean', 'sum'],\n",
        "        # 가공 컬럼\n",
        "        'PREV_CREDIT_DIFF':['mean', 'max', 'sum'],\n",
        "        'PREV_CREDIT_APPL_RATIO':['mean', 'max'],\n",
        "        'PREV_GOODS_DIFF':['mean', 'max', 'sum'],\n",
        "        'PREV_GOODS_APPL_RATIO':['mean', 'max'],\n",
        "        'PREV_DAYS_LAST_DUE_DIFF':['mean', 'max', 'sum'],\n",
        "        'PREV_INTERESTS_RATE':['mean', 'max']\n",
        "    }\n",
        "\n",
        "    prev_group = prev.groupby('SK_ID_CURR')\n",
        "    prev_amt_agg = prev_group.agg(agg_dict)\n",
        "\n",
        "    # multi index 컬럼을 '_'로 연결하여 컬럼명 변경\n",
        "    prev_amt_agg.columns = [\"PREV_\"+ \"_\".join(x).upper() for x in prev_amt_agg.columns.ravel()]\n",
        "\n",
        "    return prev_amt_agg\n",
        "\n",
        "def get_prev_refused_appr_agg(prev):\n",
        "    # 원래 groupby 컬럼 + 세부 기준 컬럼으로 groupby 수행. 세분화된 레벨로 aggregation 수행 한 뒤에 unstack()으로 컬럼레벨로 변형.\n",
        "    prev_refused_appr_group = prev[prev['NAME_CONTRACT_STATUS'].isin(['Approved', 'Refused'])].groupby([ 'SK_ID_CURR', 'NAME_CONTRACT_STATUS'])\n",
        "    prev_refused_appr_agg = prev_refused_appr_group['SK_ID_CURR'].count().unstack()\n",
        "    # 컬럼명 변경.\n",
        "    prev_refused_appr_agg.columns = ['PREV_APPROVED_COUNT', 'PREV_REFUSED_COUNT' ]\n",
        "    # NaN값은 모두 0으로 변경.\n",
        "    prev_refused_appr_agg = prev_refused_appr_agg.fillna(0)\n",
        "\n",
        "    return prev_refused_appr_agg\n",
        "\n",
        "\n",
        "\n",
        "def get_prev_agg(prev):\n",
        "    prev = get_prev_processed(prev)\n",
        "    prev_amt_agg = get_prev_amt_agg(prev)\n",
        "    prev_refused_appr_agg = get_prev_refused_appr_agg(prev)\n",
        "\n",
        "    # prev_amt_agg와 조인.\n",
        "    prev_agg = prev_amt_agg.merge(prev_refused_appr_agg, on='SK_ID_CURR', how='left')\n",
        "    # SK_ID_CURR별 과거 대출건수 대비 APPROVED_COUNT 및 REFUSED_COUNT 비율 생성.\n",
        "    prev_agg['PREV_REFUSED_RATIO'] = prev_agg['PREV_REFUSED_COUNT']/prev_agg['PREV_SK_ID_CURR_COUNT']\n",
        "    prev_agg['PREV_APPROVED_RATIO'] = prev_agg['PREV_APPROVED_COUNT']/prev_agg['PREV_SK_ID_CURR_COUNT']\n",
        "    # 'PREV_REFUSED_COUNT', 'PREV_APPROVED_COUNT' 컬럼 drop\n",
        "    prev_agg = prev_agg.drop(['PREV_REFUSED_COUNT', 'PREV_APPROVED_COUNT'], axis=1)\n",
        "\n",
        "    return prev_agg\n",
        "\n",
        "def get_apps_all_with_prev_agg(apps, prev):\n",
        "    apps_all =  get_apps_processed(apps)\n",
        "    prev_agg = get_prev_agg(prev)\n",
        "    print('prev_agg shape:', prev_agg.shape)\n",
        "    print('apps_all before merge shape:', apps_all.shape)\n",
        "    apps_all = apps_all.merge(prev_agg, on='SK_ID_CURR', how='left')\n",
        "    print('apps_all after merge with prev_agg shape:', apps_all.shape)\n",
        "\n",
        "    return apps_all\n",
        "\n",
        "def get_apps_all_encoded(apps_all):\n",
        "    object_columns = apps_all.dtypes[apps_all.dtypes == 'object'].index.tolist()\n",
        "    for column in object_columns:\n",
        "        apps_all[column] = pd.factorize(apps_all[column])[0]\n",
        "\n",
        "    return apps_all\n",
        "\n",
        "def get_apps_all_train_test(apps_all):\n",
        "    apps_all_train = apps_all[~apps_all['TARGET'].isnull()]\n",
        "    apps_all_test = apps_all[apps_all['TARGET'].isnull()]\n",
        "\n",
        "    apps_all_test = apps_all_test.drop('TARGET', axis=1)\n",
        "\n",
        "    return apps_all_train, apps_all_test\n",
        "\n",
        "def train_apps_all(apps_all_train):\n",
        "    ftr_app = apps_all_train.drop(['SK_ID_CURR', 'TARGET'], axis=1)\n",
        "    target_app = apps_all_train['TARGET']\n",
        "\n",
        "    train_x, valid_x, train_y, valid_y = train_test_split(ftr_app, target_app, test_size=0.3, random_state=2020)\n",
        "    print('train shape:', train_x.shape, 'valid shape:', valid_x.shape)\n",
        "    clf = LGBMClassifier(\n",
        "                nthread=4,\n",
        "                n_estimators=2000,\n",
        "                learning_rate=0.01,\n",
        "                num_leaves=32,\n",
        "                colsample_bytree=0.8,\n",
        "                subsample=0.8,\n",
        "                max_depth=8,\n",
        "                reg_alpha=0.04,\n",
        "                reg_lambda=0.07,\n",
        "                min_child_weight=40,\n",
        "                silent=-1,\n",
        "                verbose=-1,\n",
        "                )\n",
        "\n",
        "    clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)], eval_metric= 'auc', verbose= 100,\n",
        "                early_stopping_rounds= 100)\n",
        "\n",
        "    return clf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqqmKKhrvR4w"
      },
      "source": [
        "##### 최종 집합 생성 및 인코딩, 학습/테스트 데이터 분리, 학습/검증 피처와 타겟 데이터 분리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vsBLQ8FvR4x",
        "outputId": "17e759c8-211b-41f9-be1c-81bf3043c8a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-02f5c71367d3>:53: FutureWarning: Index.ravel returning ndarray is deprecated; in a future version this will return a view on self.\n",
            "  prev_amt_agg.columns = [\"PREV_\"+ \"_\".join(x).upper() for x in prev_amt_agg.columns.ravel()]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prev_agg shape: (338857, 41)\n",
            "apps_all before merge shape: (356255, 135)\n",
            "apps_all after merge with prev_agg shape: (356255, 176)\n"
          ]
        }
      ],
      "source": [
        "apps_all = get_apps_all_with_prev_agg(apps, prev)\n",
        "apps_all = get_apps_all_encoded(apps_all)\n",
        "apps_all_train, apps_all_test = get_apps_all_train_test(apps_all)\n",
        "ftr_app = apps_all_train.drop(['SK_ID_CURR', 'TARGET'], axis=1)\n",
        "target_app = apps_all_train['TARGET']\n",
        "train_x, valid_x, train_y, valid_y = train_test_split(ftr_app, target_app, test_size=0.3, random_state=2020)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEOB24lVvR47"
      },
      "source": [
        "#### Bayesian Optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDf92dTOw0rR",
        "outputId": "5ce1aae6-ce63-4f4d-dd61-f888f12ed5a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bayesian-optimization in /usr/local/lib/python3.10/dist-packages (1.4.3)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from bayesian-optimization) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from bayesian-optimization) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from bayesian-optimization) (1.2.2)\n",
            "Requirement already satisfied: colorama>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from bayesian-optimization) (0.4.6)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization) (3.2.0)\n"
          ]
        }
      ],
      "source": [
        "# bayesian optimization 패키지 설치\n",
        "!pip install bayesian-optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MDQVRZeRvR48"
      },
      "outputs": [],
      "source": [
        "from bayes_opt import BayesianOptimization\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from lightgbm import LGBMClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDmrYrAUvR5B"
      },
      "source": [
        "##### 함수의 입력값 search 범위(하이퍼 파라미터 별 입력 범위) 를 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NrIZzOHbvR5C"
      },
      "outputs": [],
      "source": [
        "bayesian_params = {\n",
        "    'max_depth':(6,16),\n",
        "    'num_leaves':(24,64),\n",
        "    'min_child_samples':(10, 200),\n",
        "    'min_child_weight':(1,50),\n",
        "    'subsample':(0.5, 1),\n",
        "    'colsample_bytree':(0.5, 1),\n",
        "    'max_bin':(10, 500),\n",
        "    'reg_lamda':(0.001, 10),\n",
        "    'reg_alpha':(0.01, 50)\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yog23y34vR5H"
      },
      "source": [
        "##### 최대 값을 구할 함수 선언.\n",
        "* iteration 시 마다 hyperparameter를 입력받아 classifier 학습하고 roc_auc_score값을 반환"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install lightgbm==3.3.3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "id": "o2LG7dXIeyyZ",
        "outputId": "3bf0764d-4087-4d8a-d7d4-64b53a66475e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lightgbm==3.3.3\n",
            "  Downloading lightgbm-3.3.3-py3-none-manylinux1_x86_64.whl (2.0 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/2.0 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/2.0 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from lightgbm==3.3.3) (0.42.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from lightgbm==3.3.3) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from lightgbm==3.3.3) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn!=0.22.0 in /usr/local/lib/python3.10/dist-packages (from lightgbm==3.3.3) (1.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn!=0.22.0->lightgbm==3.3.3) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn!=0.22.0->lightgbm==3.3.3) (3.2.0)\n",
            "Installing collected packages: lightgbm\n",
            "  Attempting uninstall: lightgbm\n",
            "    Found existing installation: lightgbm 4.1.0\n",
            "    Uninstalling lightgbm-4.1.0:\n",
            "      Successfully uninstalled lightgbm-4.1.0\n",
            "Successfully installed lightgbm-3.3.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "lightgbm"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YduL1j4RvR5J"
      },
      "outputs": [],
      "source": [
        "def lgb_roc_eval(max_depth, num_leaves, min_child_samples, min_child_weight, subsample,\n",
        "                 colsample_bytree, max_bin, reg_lamda, reg_alpha):\n",
        "  params = {\n",
        "      \"n_estimators\":500, \"learning_rate\":0.02,\n",
        "      'max_depth': int(round(max_depth)),\n",
        "      'num_leaves': int(round(num_leaves)),\n",
        "      'min_child_samples': int(round(min_child_samples)),\n",
        "      'min_child_weight': int(round(min_child_weight)),\n",
        "      'subsample': max(min(subsample, 0), 1),\n",
        "      'colsample_bytree': max(min(colsample_bytree, 0), 1),\n",
        "      'max_bin':  max(int(round(max_bin)),10),\n",
        "      'reg_lamda': max(reg_lamda,0),\n",
        "      'reg_alpha': max(reg_alpha, 0)\n",
        "  }\n",
        "  lgb_model = LGBMClassifier(**params)\n",
        "  lgb_model.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)], eval_metric= 'auc', verbose= 100,\n",
        "                early_stopping_rounds= 100)\n",
        "  valid_proba = lgb_model.predict_proba(valid_x)[:, 1]\n",
        "  roc_auc = roc_auc_score(valid_y, valid_proba)\n",
        "\n",
        "  return roc_auc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmnQwMudvR5Q"
      },
      "source": [
        "\n",
        "##### BayesianOptimization 객체 생성 후 함수 반환값이 최대가 되는 입력값 search를 위한 iteration 수행"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vcny5qJrvR5R",
        "outputId": "d525f55f-8d23-4827-c702-6a6b76511dd4",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|   iter    |  target   | colsam... |  max_bin  | max_depth | min_ch... | min_ch... | num_le... | reg_alpha | reg_lamda | subsample |\n",
            "-------------------------------------------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/lightgbm/sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
            "/usr/local/lib/python3.10/dist-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Unknown parameter: reg_lamda\n",
            "[100]\ttraining's auc: 0.770036\ttraining's binary_logloss: 0.245583\tvalid_1's auc: 0.754539\tvalid_1's binary_logloss: 0.248845\n",
            "[200]\ttraining's auc: 0.78881\ttraining's binary_logloss: 0.237821\tvalid_1's auc: 0.766079\tvalid_1's binary_logloss: 0.244145\n",
            "[300]\ttraining's auc: 0.800791\ttraining's binary_logloss: 0.233223\tvalid_1's auc: 0.771113\tvalid_1's binary_logloss: 0.242332\n",
            "[400]\ttraining's auc: 0.810282\ttraining's binary_logloss: 0.229703\tvalid_1's auc: 0.773541\tvalid_1's binary_logloss: 0.241496\n",
            "[500]\ttraining's auc: 0.818629\ttraining's binary_logloss: 0.226638\tvalid_1's auc: 0.775144\tvalid_1's binary_logloss: 0.240941\n",
            "| \u001b[0m1        \u001b[0m | \u001b[0m0.7751   \u001b[0m | \u001b[0m0.7744   \u001b[0m | \u001b[0m360.4    \u001b[0m | \u001b[0m12.03    \u001b[0m | \u001b[0m113.5    \u001b[0m | \u001b[0m21.76    \u001b[0m | \u001b[0m49.84    \u001b[0m | \u001b[0m21.88    \u001b[0m | \u001b[0m8.918    \u001b[0m | \u001b[0m0.9818   \u001b[0m |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/lightgbm/sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
            "/usr/local/lib/python3.10/dist-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Unknown parameter: reg_lamda\n",
            "[100]\ttraining's auc: 0.761967\ttraining's binary_logloss: 0.247269\tvalid_1's auc: 0.7529\tvalid_1's binary_logloss: 0.249031\n",
            "[200]\ttraining's auc: 0.780705\ttraining's binary_logloss: 0.240193\tvalid_1's auc: 0.765823\tvalid_1's binary_logloss: 0.244171\n",
            "[300]\ttraining's auc: 0.791651\ttraining's binary_logloss: 0.236144\tvalid_1's auc: 0.771318\tvalid_1's binary_logloss: 0.242226\n",
            "[400]\ttraining's auc: 0.800034\ttraining's binary_logloss: 0.23312\tvalid_1's auc: 0.774304\tvalid_1's binary_logloss: 0.241165\n",
            "[500]\ttraining's auc: 0.806854\ttraining's binary_logloss: 0.23064\tvalid_1's auc: 0.775813\tvalid_1's binary_logloss: 0.240642\n",
            "| \u001b[95m2        \u001b[0m | \u001b[95m0.7758   \u001b[0m | \u001b[95m0.6917   \u001b[0m | \u001b[95m397.9    \u001b[0m | \u001b[95m11.29    \u001b[0m | \u001b[95m117.9    \u001b[0m | \u001b[95m46.35    \u001b[0m | \u001b[95m26.84    \u001b[0m | \u001b[95m4.366    \u001b[0m | \u001b[95m0.2032   \u001b[0m | \u001b[95m0.9163   \u001b[0m |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/lightgbm/sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
            "/usr/local/lib/python3.10/dist-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Unknown parameter: reg_lamda\n",
            "[100]\ttraining's auc: 0.776586\ttraining's binary_logloss: 0.243269\tvalid_1's auc: 0.757064\tvalid_1's binary_logloss: 0.247793\n",
            "[200]\ttraining's auc: 0.798892\ttraining's binary_logloss: 0.234191\tvalid_1's auc: 0.768695\tvalid_1's binary_logloss: 0.24313\n",
            "[300]\ttraining's auc: 0.8148\ttraining's binary_logloss: 0.228188\tvalid_1's auc: 0.773355\tvalid_1's binary_logloss: 0.241537\n",
            "[400]\ttraining's auc: 0.828183\ttraining's binary_logloss: 0.22311\tvalid_1's auc: 0.776125\tvalid_1's binary_logloss: 0.240574\n",
            "[500]\ttraining's auc: 0.839348\ttraining's binary_logloss: 0.218891\tvalid_1's auc: 0.777196\tvalid_1's binary_logloss: 0.24021\n",
            "| \u001b[95m3        \u001b[0m | \u001b[95m0.7772   \u001b[0m | \u001b[95m0.8891   \u001b[0m | \u001b[95m436.3    \u001b[0m | \u001b[95m15.79    \u001b[0m | \u001b[95m161.8    \u001b[0m | \u001b[95m23.61    \u001b[0m | \u001b[95m55.22    \u001b[0m | \u001b[95m5.923    \u001b[0m | \u001b[95m6.4      \u001b[0m | \u001b[95m0.5717   \u001b[0m |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/lightgbm/sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
            "/usr/local/lib/python3.10/dist-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Unknown parameter: reg_lamda\n",
            "[100]\ttraining's auc: 0.765885\ttraining's binary_logloss: 0.246869\tvalid_1's auc: 0.753465\tvalid_1's binary_logloss: 0.249299\n",
            "[200]\ttraining's auc: 0.783293\ttraining's binary_logloss: 0.239682\tvalid_1's auc: 0.765115\tvalid_1's binary_logloss: 0.24457\n",
            "[300]\ttraining's auc: 0.793759\ttraining's binary_logloss: 0.235697\tvalid_1's auc: 0.770167\tvalid_1's binary_logloss: 0.24274\n",
            "[400]\ttraining's auc: 0.801654\ttraining's binary_logloss: 0.232743\tvalid_1's auc: 0.772977\tvalid_1's binary_logloss: 0.24176\n",
            "[500]\ttraining's auc: 0.808487\ttraining's binary_logloss: 0.230232\tvalid_1's auc: 0.774704\tvalid_1's binary_logloss: 0.241165\n",
            "| \u001b[0m4        \u001b[0m | \u001b[0m0.7747   \u001b[0m | \u001b[0m0.9723   \u001b[0m | \u001b[0m265.7    \u001b[0m | \u001b[0m10.15    \u001b[0m | \u001b[0m60.27    \u001b[0m | \u001b[0m38.94    \u001b[0m | \u001b[0m42.25    \u001b[0m | \u001b[0m28.43    \u001b[0m | \u001b[0m0.1889   \u001b[0m | \u001b[0m0.8088   \u001b[0m |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/lightgbm/sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
            "/usr/local/lib/python3.10/dist-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Unknown parameter: reg_lamda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "KeyboardInterrupt\n",
            "\n"
          ]
        }
      ],
      "source": [
        "lgbBO = BayesianOptimization(f=lgb_roc_eval, pbounds=bayesian_params, random_state=0)\n",
        "lgbBO.maximize(init_points=5, n_iter=25)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2L3qCmiXvR5a"
      },
      "source": [
        "##### Iteration 수행 결과 출력"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DinJ-PaxvR5c"
      },
      "outputs": [],
      "source": [
        "# BayesianOptimization객체의 res는 iteration 수행 시마다 모든 함수 반환결과와 그때의 파라미터 결과값을 가지고 있음.\n",
        "lgbBO.res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pF714uFvR5l"
      },
      "source": [
        "##### Iteration 결과 Dictionary에서 최대 target값을 가지는 index 추출하고 그때의 parameter 값을 추출.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aR-_KsyivR5o"
      },
      "outputs": [],
      "source": [
        "# dictionary에 있는 target값을 모두 추출\n",
        "target_list = []\n",
        "for result in lgbBO.res:\n",
        "    target = result['target']\n",
        "    target_list.append(target)\n",
        "print(target_list)\n",
        "# 가장 큰 target 값을 가지는 순번(index)를 추출\n",
        "print('maximum target index:', np.argmax(np.array(target_list)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZLxoImyvR5x"
      },
      "outputs": [],
      "source": [
        "# 가장 큰 target값을 가지는 index값을 기준으로 res에서 해당 parameter 추출.\n",
        "max_dict = lgbBO.res[np.argmax(np.array(target_list))]\n",
        "print(max_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQ0EHnuIvR53"
      },
      "source": [
        "##### 최적화된 하이퍼 파라미터를 기반으로 재 테스트"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32l2G2bZvR54"
      },
      "outputs": [],
      "source": [
        "def train_apps_all(apps_all_train):\n",
        "    ftr_app = apps_all_train.drop(['SK_ID_CURR', 'TARGET'], axis=1)\n",
        "    target_app = apps_all_train['TARGET']\n",
        "\n",
        "    train_x, valid_x, train_y, valid_y = train_test_split(ftr_app, target_app, test_size=0.3, random_state=2020)\n",
        "    print('train shape:', train_x.shape, 'valid shape:', valid_x.shape)\n",
        "    clf = LGBMClassifier(\n",
        "                nthread=4,\n",
        "                n_estimators=1000,\n",
        "                learning_rate=0.02,\n",
        "                max_depth = 13,\n",
        "                num_leaves=57,\n",
        "                colsample_bytree=0.638,\n",
        "                subsample=0.682,\n",
        "                max_bin=435,\n",
        "                reg_alpha=0.936,\n",
        "                reg_lambda=4.533,\n",
        "                min_child_weight=25,\n",
        "                min_child_samples=166,\n",
        "                silent=-1,\n",
        "                verbose=-1,\n",
        "                )\n",
        "\n",
        "    clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)], eval_metric= 'auc', verbose= 100,\n",
        "                early_stopping_rounds= 100)\n",
        "\n",
        "    return clf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxqV8KpFvR5_",
        "outputId": "ea1c8010-a141-4c0c-fdc0-8f56c04ff380"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-02f5c71367d3>:53: FutureWarning: Index.ravel returning ndarray is deprecated; in a future version this will return a view on self.\n",
            "  prev_amt_agg.columns = [\"PREV_\"+ \"_\".join(x).upper() for x in prev_amt_agg.columns.ravel()]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prev_agg shape: (338857, 41)\n",
            "apps_all before merge shape: (356255, 135)\n",
            "apps_all after merge with prev_agg shape: (356255, 176)\n",
            "train shape: (215257, 174) valid shape: (92254, 174)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/lightgbm/sklearn.py:598: UserWarning: 'silent' argument is deprecated and will be removed in a future release of LightGBM. Pass 'verbose' parameter via keyword arguments instead.\n",
            "  _log_warning(\"'silent' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
            "/usr/local/lib/python3.10/dist-packages/lightgbm/sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
            "/usr/local/lib/python3.10/dist-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
            "  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[100]\ttraining's auc: 0.780153\ttraining's binary_logloss: 0.242981\tvalid_1's auc: 0.759911\tvalid_1's binary_logloss: 0.24756\n",
            "[200]\ttraining's auc: 0.801589\ttraining's binary_logloss: 0.233556\tvalid_1's auc: 0.769847\tvalid_1's binary_logloss: 0.242803\n",
            "[300]\ttraining's auc: 0.817628\ttraining's binary_logloss: 0.227376\tvalid_1's auc: 0.773659\tvalid_1's binary_logloss: 0.241322\n",
            "[400]\ttraining's auc: 0.831406\ttraining's binary_logloss: 0.222166\tvalid_1's auc: 0.776379\tvalid_1's binary_logloss: 0.240369\n",
            "[500]\ttraining's auc: 0.84293\ttraining's binary_logloss: 0.217731\tvalid_1's auc: 0.777363\tvalid_1's binary_logloss: 0.240028\n",
            "[600]\ttraining's auc: 0.853247\ttraining's binary_logloss: 0.21375\tvalid_1's auc: 0.777921\tvalid_1's binary_logloss: 0.239843\n",
            "[700]\ttraining's auc: 0.862535\ttraining's binary_logloss: 0.210085\tvalid_1's auc: 0.77821\tvalid_1's binary_logloss: 0.239772\n",
            "[800]\ttraining's auc: 0.871324\ttraining's binary_logloss: 0.20648\tvalid_1's auc: 0.778582\tvalid_1's binary_logloss: 0.239664\n",
            "[900]\ttraining's auc: 0.879739\ttraining's binary_logloss: 0.203013\tvalid_1's auc: 0.778513\tvalid_1's binary_logloss: 0.239672\n"
          ]
        }
      ],
      "source": [
        "apps_all = get_apps_all_with_prev_agg(apps, prev)\n",
        "apps_all = get_apps_all_encoded(apps_all)\n",
        "apps_all_train, apps_all_test = get_apps_all_train_test(apps_all)\n",
        "clf = train_apps_all(apps_all_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gQrAcQ1QvR6I"
      },
      "outputs": [],
      "source": [
        "preds = clf.predict_proba(apps_all_test.drop(['SK_ID_CURR','TARGET'], axis=1))[:, 1 ]\n",
        "apps_all_test['TARGET'] = preds\n",
        "# SK_ID_CURR과 TARGET 값만 csv 형태로 생성. 코랩 버전은 구글 드라이브 절대 경로로 입력\n",
        "default_dir = \"/content/gdrive/My Drive\"\n",
        "apps_all_test[['SK_ID_CURR', 'TARGET']].to_csv(os.path.join(default_dir,'prev_baseline_tuning_01.csv'), index=False)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l01RNW9nvR6y"
      },
      "source": [
        "##### cross validation 으로 hyper parameter 재 tuning"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NmUaDNKvZuJl"
      },
      "outputs": [],
      "source": [
        "bayesian_params = {\n",
        "    'max_depth': (6, 16),\n",
        "    'num_leaves': (24, 64),\n",
        "    'min_data_in_leaf': (10, 200), # min_child_samples\n",
        "    'min_child_weight':(1, 50),\n",
        "    'bagging_fraction':(0.5, 1.0), # subsample\n",
        "    'feature_fraction': (0.5, 1.0), # colsample_bytree\n",
        "    'max_bin':(10, 500),\n",
        "    'lambda_l2':(0.001, 10), # reg_lambda\n",
        "    'lambda_l1': (0.01, 50) # reg_alpha\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qnhm1sL_ZuJm"
      },
      "outputs": [],
      "source": [
        "import lightgbm as lgb\n",
        "\n",
        "train_data = lgb.Dataset(data=ftr_app, label=target_app, free_raw_data=False)\n",
        "def lgb_roc_eval_cv(max_depth, num_leaves, min_data_in_leaf, min_child_weight, bagging_fraction,\n",
        "                 feature_fraction,  max_bin, lambda_l2, lambda_l1):\n",
        "    params = {\n",
        "        \"num_iterations\":500, \"learning_rate\":0.02,\n",
        "        'early_stopping_rounds':100, 'metric':'auc',\n",
        "        'max_depth': int(round(max_depth)), #  호출 시 실수형 값이 들어오므로 실수형 하이퍼 파라미터는 정수형으로 변경\n",
        "        'num_leaves': int(round(num_leaves)),\n",
        "        'min_data_in_leaf': int(round(min_data_in_leaf)),\n",
        "        'min_child_weight': int(round(min_child_weight)),\n",
        "        'bagging_fraction': max(min(bagging_fraction, 1), 0),\n",
        "        'feature_fraction': max(min(feature_fraction, 1), 0),\n",
        "        'max_bin':  max(int(round(max_bin)),10),\n",
        "        'lambda_l2': max(lambda_l2,0),\n",
        "        'lambda_l1': max(lambda_l1, 0)\n",
        "    }\n",
        "    # 파이썬 lightgbm의 cv 메소드를 사용.\n",
        "\n",
        "    cv_result = lgb.cv(params, train_data, nfold=3, seed=0,  verbose_eval =100,  early_stopping_rounds=50, metrics=['auc'])\n",
        "    return max(cv_result['auc-mean'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGlWbTX4ZuJn"
      },
      "outputs": [],
      "source": [
        "max_dict = lgbBO.res[np.argmax(np.array(target_list))]\n",
        "print(max_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69mqcBCBZuJo"
      },
      "outputs": [],
      "source": [
        "def train_apps_all(apps_all_train):\n",
        "    ftr_app = apps_all_train.drop(['SK_ID_CURR', 'TARGET'], axis=1)\n",
        "    target_app = apps_all_train['TARGET']\n",
        "\n",
        "    train_x, valid_x, train_y, valid_y = train_test_split(ftr_app, target_app, test_size=0.3, random_state=2020)\n",
        "    print('train shape:', train_x.shape, 'valid shape:', valid_x.shape)\n",
        "    clf = LGBMClassifier(\n",
        "                nthread=4,\n",
        "                n_estimators=1000,\n",
        "                learning_rate=0.02,\n",
        "                max_depth = 10,\n",
        "                num_leaves=60,\n",
        "                colsample_bytree=0.511,\n",
        "                subsample=0.785,\n",
        "                max_bin=208,\n",
        "                reg_alpha=7.009,\n",
        "                reg_lambda=6.579,\n",
        "                min_child_weight=40,\n",
        "                min_child_samples=91,\n",
        "                silent=-1,\n",
        "                verbose=-1,\n",
        "                )\n",
        "\n",
        "    clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)], eval_metric= 'auc', verbose= 100,\n",
        "                early_stopping_rounds= 100)\n",
        "\n",
        "    return clf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OvFSprAwZuJp"
      },
      "outputs": [],
      "source": [
        "apps_all = get_apps_all_with_prev_agg(apps, prev)\n",
        "apps_all = get_apps_all_encoded(apps_all)\n",
        "apps_all_train, apps_all_test = get_apps_all_train_test(apps_all)\n",
        "clf = train_apps_all(apps_all_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eLju24HsZuJp"
      },
      "outputs": [],
      "source": [
        "preds = clf.predict_proba(apps_all_test.drop('SK_ID_CURR', axis=1))[:, 1 ]\n",
        "apps_all_test['TARGET'] = preds\n",
        "# SK_ID_CURR과 TARGET 값만 csv 형태로 생성. 코랩 버전은 구글 드라이브 절대 경로로 입력\n",
        "default_dir = \"/content/gdrive/My Drive\"\n",
        "app_test[['SK_ID_CURR', 'TARGET']].to_csv(os.path.join(default_dir,'prev_baseline_tuning_02.csv'), index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}